\documentclass{ieeeojies}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{natbib}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}
\title{Forecasting the Motor Brands stock prices in Japan: Honda, Yamaha, Suzuki using Statistical, Machine Learning, and Deep Learning Models (2018 - 2023)}

\author{{\uppercase{Pham Thi Thuy Trang}\authorrefmark{1} \IEEEmembership{Leader},\uppercase{Tran Ngoc Khoi Nguyen} \authorrefmark{2}\IEEEmembership{Member}, and Chau Nguyen Thanh Tai}\authorrefmark{3} \IEEEmembership{Member}. \textcolor{red}}

\address[1]{Pham Thi Thuy Trang, CTTT2021, University of Information Technology HCMC, Vietnam, (gmail: 21522697@gm.uit.edu.vn)}
\address[2]{Tran Ngoc Khoi Nguyen, CTTT2021, University of Information Technology HCMC, Vietnam, (gmail: 21522398@gm.uit.edu.vn)}
\address[3]{Chau Nguyen Thanh Tai, CTTT2021, University of Information Technology HCMC, Vietnam, (gmail: 21522561@gm.uit.edu.vn)}

\markboth
{Author \headeretal: Trang P. T. T., Nguyen T. N. K., Tai C. N. T.}
{Author \headeretal: Trang P. T. T., Nguyen T. N. K., Tai C. N. T.}

\begin{abstract}
This report presents a comparative analysis of eight models,
including Linear Regression, Autoregressive Integrated Moving Average (ARIMA), Support Vector Regression (SVR), Recurrent Neural Network (RNN), Garch, Convolutional Neural Network and Gated Recurrent Unit (CNN - GRU), Transformer, Deep Neural Network Models (DNN) for predicting the Motor Brands stock prices in Japan. The models are evaluated using MAE, MAPE, and RMSE metrics on historical motor price indicators data. The model with the lowest MAE, MAPE, and RMSE is recommended for motor price forecasting, contributing to improved understanding and accurate predictions in the motor
market.
\end{abstract}

\begin{keywords}
\textbf{Keywords:} motor price, forecasting, linear regression, ARIMA, SVR, RNN, Garch, CNN-GRU, Transformer and DNN.
\end{keywords}

\titlepgskip=-15pt

\maketitle

\section{Introduction}
\label{sec:introduction}
Honda, Yamha and Suzuki are all Japanese multinational car manufacturing companies. Their products are very diverse, including: motorbikes, engine components, electrical equipment,... However, each company has different short-term products but all aim at the same goal. They want to understand the future needs and tastes of customers and what new products they want to have. Additionally, Innovation in design structure, production and consumer interaction aims to keep up with the trending needs of the current market. All three companies have good growth potential as well as many potential risks in the future. Therefore, predicting the stock prices of these companies will help consultants make decisions and choices in accordance with the agency's investment policy. The article applies 8 different algorithms and builds models to help predict the stock prices of these companies with the goal of finding the model that is best able to predict future stock values as well as trends its increase or decrease.

\section{Related Works}
\label{sec:related works} 
\bfseries Linear Regression:
\mdseries Bhawna Panwar et al used two models, Linear Regression and SVM (State Vector Machine) to predict and evaluate these two models. With the dataset being Amazon's stock price in October 2019. The results show that the accuracy of Linear Regression is 98.76\%, higher than the accuracy of SVR is 94.32\%. \cite{related_lr}
\\
\bfseries Arima:
\mdseries Hyeong Kyu Choi, a B.A student in Korea, applied the unified ARIMA-LSTM model in predicting the stock price correlation coefficient of two individual shares. to filter out the linear feature in each step of the ARIMA model, then expect the non-linear trend in the recurrent neural LSTM growing network. Experimental results show that the ARIMA-LSTM model significantly outperforms other equivalent financial models. Model performance has been validated over various time periods and combinations of assets with metrics such as MSE, RMSE, and MAE. Based on this superiority, it can be assumed that the ARIMA-LSTM hybrid model has enough expected potential. Therefore, the ARIMA-LSTM model as a predicted correlation number system for item prioritization can be considered. \cite{related_arima}\\
\bfseries SVR:
\mdseries Youshan Zhang and Qi Li had proposed a regressive convolution neural network and support vector regression (RCNN-SVR) model for electricity consumption forecasting. According to the experimental results, the RCNN-SVR model can precisely predict electricity consumption in the next following months. Also, the proposed model is compared with four models that were used in electricity consumption forecasting. The comparison results showed that performance of our RCNN-SVR model is the best among all tested algorithms, which has the lowest values of MSE, MAPE, and CV-RMSE. \cite{related_svr}\\
\bfseries RNN:
\mdseries Saeed Khaki, Lizhi Wang, Sotirios V. Archontoulis presented a deep learning framework using convolutional neural networks (CNN) and recurrent neural networks (RNN) for crop yield prediction based on environmental data and management practices. The proposed CNN-RNN model, along with other popular methods such as random forest (RF), deep fully-connected neural networks (DFNN), and LASSO, was used to forecast corn and soybean yield across the entire Corn Belt (including 13 states) in the United States for years 2016, 2017, and 2018 using historical data. The new model achieved a root-mean-square-error (RMSE) 9\% and 8\% of their respective average yields, substantially outperforming all other methods that were tested. \cite{related_rnn}
\\
\bfseries Garch:
\mdseries MSc. Nguyen Thi Hien, PhD. Dang Thi Minh Nguyet, Tran Thi Lan, Khuat Thi Vy, Tran Thi Linh of the University of Commerce in Vietnam chose the ARIMA(1,1,2), ARCH (3), GARCH (2,1) model. , GARCH (2,1)-M, TGARCH (2,1) to analyze the return rate fluctuations of the VN30F1M index with the data set during the period from August 2017 to September 2021. The study shows that the GARCH model is superior for making forecasts for the conditional variance of returns, this result is similar to previous results of Karmakar (2007), Goudarzi (2010), Sohail Chand, Shahid Kamal and Imran Ali (2012), Tran Sy Manh and Do Khac Huong (2013), Pham Chi Khoa (2017). The results of the GARCH (2,1) model show that the past rate of return determines the current rate of return and the fluctuation of the current rate of return but is affected by a decrease in the past, when there is past shocks, the return rate of the VN30F1M hybrid could change by about 13.33\%. \cite{related_garch}
\\
\bfseries CNN-GRU:
\mdseries Deepak Gupta, Pabitra Lenka, Harsimran Bedi, Asif Ekbal, Pushpak Bhattacharyya had proposed deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) based approaches that do not require handcrafting of features. They evaluate these techniques for analyzing customer feedback sentences in four languages, namely English, French, Japanese and Spanish. Our empirical analysis shows that our models perform well in all the four languages on the setups of IJCNLP Shared Task on Customer Feedback Analysis. Our model achieved the second rank in French, with an accuracy of 71.75\% and third ranks for all the other languages. \cite{related_cnngru}
\\
\bfseries Transformer:
\mdseries Minghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei had presented TrOCR, an end-to-end Transformer-based OCR model for text recognition with pre-trained models. Distinct from existing approaches, TrOCR does not rely on the conventional CNN models for image understanding. Instead, it leverages an image Transformer model as the visual encoder and a text Transformer model as the textual decoder. Experiment results show that TrOCR achieves state-of-the-art results on printed, handwritten and scene text recognition with just a simple encoder-decoder model, without any post-processing steps. \cite{related_transformer}
\\
\bfseries DNN:
\mdseries Fei Xu, Jianian Xu, Jiabin Chen, Li Chen, Ruitao Shang, Zhi Zhou, Fangming Liu presented iGniter, an interference-aware GPU resource provisioning framework for predictable DNN inference in the cloud. Through a lightweight analytical performance model leveraging key metrics, they capture interference in co-located GPU inference workloads. iGniter optimizes GPU resource allocation and batch size to minimize interference, achieving a cost-efficient provisioning strategy. Extensive experiments on Amazon EC2 demonstrate iGniter's ability to meet performance Service Level Objectives (SLOs) for DNN inference workloads while reducing costs by up to 25\% compared to state-of-the-art strategies. \cite{related_dnn} 
\section{MODELLING}
\label{sec:modelling}
\subsection{Data Preparation}
\textbf{a) Data sources:}
This study schange daily historical data of three popular
motor brand which are Honda, Suzuki, and Yamaha in Japan from January 04, 2018, to December 14, 2023, such as High, Low, Open, Price, Volume, and Change where:\\
• High: This represents the highest price that a motor brand reaches during the day.\\
• Low: This represents the lowest price that a motor brand reaches during the day.\\
• Open: This represents the price of a motor brand at the beginning of the day.\\
• Price: This represents the current price of a motor brand. By the end of the day, this will be known as the closing price.\\
• Volume: This represents the total number of units of a motor brand that were traded during the day.\\
• Change: This represents the percentage change in the price of a motor brand compared to its previous price. A positive change shows a price increase, while a negative change shows a price decrease.\\
\textbf{b) Descriptive Statistics:}\\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Honda_des.png}
    \caption{Descriptive statistics of Honda}
    \label{fig:enter-label}
\end{figure}\\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Suzuki_des.png}
    \caption{Descriptive statistics of Suzuki}
    \label{fig:enter-label}
\end{figure}\\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Yamaha_des.png}
    \caption{Descriptive statistics of Yamaha}
    \label{fig:enter-label}
\end{figure}\\

\textbf{c) Visualization:}
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Honda.png}
    \caption{Visualization of Honda}
    \label{fig:enter-label}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Suzuki.png}
    \caption{Visualization of Suzuki}
    \label{fig:enter-label}
\end{figure}
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Yamaha.png}
    \caption{Visualization of Yamaha}
    \label{fig:enter-label}
\end{figure}
\\
\\
\\
\\
\\
\\
\subsection{Linear Regression}
A straightforward yet widely used prediction method is linear regression. Based on the linear relationship between the independent input variables and the dependent variable that needs to be predicted, it makes a value prediction. Simple linear regression occurs when there is just one dependent and one independent variable in the linear regression model. Multi-linear regression is the term used when more than one dependent and independent variable is used \cite{tai2}. The linear regression formula is:
\begin{equation}
y=\beta_0+{\beta_1}X_1+{\beta_2}X_2+{\beta_3}X_3+...+{\beta_k}X_k+\epsilon
\end{equation} \cite{tai2}
Where:\\
\textbullet \ Y is the dependent variable\\
\textbullet \ X1, X2, X3, ... are the independent variables\\
\textbullet \ $\beta0$ is the intercept term\\
\textbullet \ $\beta1, \beta2, \beta3, ...$ is the coefficient of the independent variables\\
\textbullet \ $\epsilon$ is the error term\\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Overview_LR.png}
    \caption{Overview of Linear Regression}
    \label{fig:enter-label}
\end{figure}
\subsection{Arima}
ARIMA stands for Autoregressive Integrated Moving Average. It is a statistical forecasting method widely used in time series analysis. This model incorporates autoregressive (AR), moving average (MA), and differencing (I) components to capture the relationship between current and past values of a time series. \\
\textbf{Autoregressive (AR):} \\
AR (Auto Regression) - is the number of autoregressive terms. The equation for AR (p) model is: :
\begin{equation}
y_t = c + {\varphi_1}y_{t-1} + {\varphi_2}y_{t-2} + ... +  {\varphi_p}y_{t-p} + \epsilon_t
\end{equation} 
Where:\\
\textbullet \ ${y_t}$ is the current value\\
\textbullet \ c is the constant term\\
\textbullet \ $p$ is the number of orders\\
\textbullet \ ${\varphi}$ is the autoregressive coefficient\\
\textbullet \ ${\epsilon_t}$ is the error term\\
\textbf{Moving Average (MA): }\\
MA (Moving Average) - is the number of terms in the moving average. The equation for MA (q) model is: 
\begin{equation}
y_t = c + {\theta_1}\epsilon_{t-1} + {\theta_2}\epsilon_{t-2} + ... +  {\theta_p}\epsilon_{t-p} + \epsilon_t
\end{equation} 
\textbf{Differencing (I): }\\
Last, the I part is Integrated, and d is the number of differences (order) required to make it a stationary sequence. For example:\\
If d = 0: ${\Delta Y_t} = Y_t$ \\
If d = 1: ${\Delta Y_t} = Y_t - Y_{t-1}$ \\
If d = 2: ${\Delta Y_t} = (Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2}) =  Y_t - 2Y_{t-1} + Y_{t-2}$ \\
After combining them, we will have the ARIMA (p, d, q) express as follow:\\
$\Delta Y_t = c + {\varphi_1}y_{t-1} + {\varphi_2}y_{t-2} + ... +$
\begin{equation}
  {\varphi_p}y_{t-p} + {\theta_1}\epsilon_{t-1} + {\theta_2}\epsilon_{t-2} + ... +  {\theta_p}\epsilon_{t-p} + \epsilon_t
\end{equation} 
In summary, ARIMA models capture the relationship between current and past values of a time series, incorporating autoregressive, moving average, and differencing components. They are widely used for forecasting future values of various time series, including economic data, stock prices, and weather patterns.
\subsection{SVR}
Support vector regression is a popular choice for prediction and curve fitting for both linear and nonlinear regression types. SVR is based on the elements of Support vector machine (SVM), where support vectors are basically closer points towards the generated hyperplane in an n-dimensional feature space that distinctly segregates the data points about the hyperplane. The SVR model performs the fitting as shown in Figure 8. The generalized equation for hyperplane may be represented as y = wX + b, where w is weights and b are the intercept at X = 0. The margin of tolerance is represented by epsilon $\epsilon$. \cite{nguyen3} \\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Support_SVR.jpg}
    \caption{Support vector regression model for linear regression fitting where X1= X and X2 = y}
    \label{fig:enter-label}
\end{figure}
\begin{equation}
    minimize {\frac{1}{2}}\|w\|^2+C{\displaystyle \sum_{i=1}^{n}}({\xi_i}^*+\xi_i)
\end{equation}
\begin{equation}
    \begin{cases}
        y_1+\langle w,x_i \rangle-b& \le \epsilon + \xi_i^*\\ 
        \langle w,x_i \rangle + b& \le \epsilon + \xi_i
    \end{cases}
\end{equation}

Where: w is the learned weight vector, $x_i$ is the i-th training instance, $y_i$ is the training label, and $\xi_i$ is the distance between the bounds and predicted values outside the bounds. C is another parameter set by the user that is a constraint that controls the penalty imposed on observations outside the bounds that helps to prevent overﬁtting. \cite{nguyen4}
\subsection{RNN}
A Recurrent Neural Network (RNN) is a type of artificial neural network designed for sequential data processing. Commonly used in applications like language translation, NLP, speech recognition, and image captioning, RNNs differ from traditional networks by incorporating "memory." This allows them to consider prior inputs when generating current outputs, making them suitable for tasks where sequence order matters. Despite their effectiveness, unidirectional RNNs struggle to incorporate information from future events in their predictions\cite{tai3}.
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{RNN.png}
    \caption{Overview RNN}
    \label{fig:enter-label}
\end{figure}
\subsection{GARCH}
The GARCH (Generalized AutoRegressive Conditional Heteroskedasticity) model is a statistical model that describes how the variance of error in a time series depends on past errors and variances. Unlike simple models that assume constant variance (homoskedasticity), GARCH allows for volatility, which means that periods of high and low volatility can change based on historical data.\\
The GARCH model has two main components:\\
1.	\textbf{Mean equation:} Modeling the actual values of time series, usually using the process of multiplicity regression (ARMA).\\
2.	\textbf{Variance equation:} Modeling the conditional variance of error, which usually depends on past squared errors (ARCH) and past variances (GARCH).\\
\textbf{Average equation:}\\
The average equation of a GARCH model is usually modeled using an ARMA process. For example, an average equation ARMA(1.1) might be written as follows:
\begin{equation}
    y_t = c + \alpha_1 y_{t-1} + \beta_1 \epsilon_{t-1} + \epsilon_t
\end{equation} \cite{tranggarch3}
Where:\\
\textbullet \ ${y_t}$ is the value of the time series at time t\\
\textbullet \ c is the constant term\\
\textbullet \ $\alpha_1 and \beta_1$ are auto-regression coefficients and multiples\\
\textbullet \ ${\epsilon_t}$ is the error term\\
\textbf{Equation of variance:}\\
The variance equation of the GARCH model is usually written as follows:
\begin{equation}
    \sigma_t^2 = \omega + \alpha_1 \epsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2
\end{equation} \cite{tranggarch2}
\subsection{CNN-GRU}
The CNN-GRU model is proposed as a hybrid model to provide more accurate and certain forecasting results to relieve the high variability and uncertainty of the price forecasting problem.\\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{CNN_GRU.png}
    \caption{Overview of the CNN-LSTM and CNN-GRU hybrid model architecture}
    \label{fig:enter-label}
\end{figure}
In the figure, the model will utilize CNN to extract local n-gram features (where n is set by the length of the filters). The CNN’s max pooling layer down samples the output to reduce the dimensionality, which also contributes to the reduction in overfitting. The LSTM or GRU layers are then used to capture long-range dependencies that may be present within the features encoded by the CNN layers. The vectors output by the GRU layer with the context and dependencies information will then be transmitted to dense layers for further processing before the final classification by the sigmoid-activated output layer consisting of a single unit.  \cite{nguyen1}\\
It can be easier to visualize with the flowchart of the CNN-GRU algorithm as follows:
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{CNN_GRU2.jpg}
    \caption{The solution flowchart of proposed CNN-GRU hybrid Model\cite{nguyen2}}
    \label{fig:enter-label}
\end{figure}
Then applied to the intended use of this report, which is price prediction, the process of this model works as follows:
The flowchart in Figure 11 illustrates the solution process. The model consists of two main parts: the CNN for feature extraction and the GRU for forecasting. The CNN part includes three Conv1D layers and one Max1D layer, which extract spatial features from the data. The GRU part includes one GRU layer and one Dropout layer, which analyze the features extracted by the CNN and estimate the household power load for the next time period.\\
\textbf{Explanation:}
The overall process of the CNN-GRU model for price prediction is as follows. First, the spatial features in the time series data are extracted using convolution and pooling layers in the CNN. These layers help to identify patterns and important features in the data. Next, the temporal feature of the data is modeled using the GRU (Gated Recurrent Unit).
Once the features are learned from the model, they can be used to predict future prices. In the testing phase, the model takes the input data and predicts the price for the next time period. The predicted values are represented as an interval to account for the uncertainty in the predictions.\\
\subsection{Transformer}
A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. Transformer models apply an evolving set of mathematical techniques, called attention or self-attention, to detect subtle ways even distant data elements in a series influence and depend on each other. First described in a 2017 paper from Google, transformers are among the newest and one of the most powerful classes of models invented to date. They’re driving a wave of advances in machine learning some have dubbed transformer AI. OpenAI’s popular ChatGPT text generation tool makes use of transformer architectures for prediction, summarization, question answering, and more, because they allow the model to focus on the most relevant segments of input text. The “GPT” seen in the tool’s various versions (e.g. GPT-2, GPT-3) stands for “generative pre-trained transformer \cite{tai1}.”
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Transformer.png}
    \caption{Overview Transformer}
    \label{fig:enter-label}
\end{figure}
\subsection{DNN}
A DNN model comprises three layers: Input, hidden, and output. Figure a depicts the DNN- structure model. Each layer comprises multiple nodes connected hierarchically to all nodes in the subsequent layer. The input and output layers are, in general, single layers, whereas the hidden layer may comprise two or more layers. Data features are fed to the input layer, and prediction values are derived from the output layer after processing them in the hidden layers. Figure b describes the prediction principle of a DNN model. The weighted sum of nodes is calculated and predicted values are derived using an activation function that exists within each hidden-layer node that receives the weighted sum of nodes as input and converts them into valid values. The most commonly used activation function in regression analysis is the rectified linear unit (ReLU), which produces a value equal to the input if the weighted sum of nodes exceeds or equals zero; otherwise, it yields a value of zero.\\
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{DNN.png}
    \caption{Description of deep neural network (DNN) model: 
    (a) Typical structure of DNN model; 
    (b) principle of value prediction using DNN model.}
    \label{fig:enter-label}
\end{figure}
ReLU predicts values by repeatedly modifying the weight during each DNN-model training. This weight modification is performed in reverse from the output layer to the input layer via back propagation until the cost function is minimized. The cost function can be expressed as the sum of the squares of differences between the observed and predicted values. Equation (1) defines the relation to evaluate the cost function (E). Here, n refers to the number of output-layer nodes, and $y_k$ and ${\hat{y}}_k$ denote the observed and predicted values of the kth output node, respectively. Equation (2) adjusts the weight in such a way that the difference between the previous weight and partial derivative of the error function can be assumed as the next weight. Here, $n$ denotes the node number in the previous layer, $j$ denotes the node number in the next layer, $w_{ij}^t$ denotes the weight at time t, and $eta$ denotes the learning rate. \cite{nguyen5}\\
\begin{equation}
    E = \displaystyle \sum_{k=1}^{n}\left(y_k-{\hat{y}}_k\right)^2
\end{equation}
\begin{equation}
    \ w_{ij}^t=\ \ w_{ij}^t-\ \eta\frac{\partial E}{\partial w_{ij}^t}
\end{equation}
\subsection{Experiment}
\textbf{a) Dataset splitting:}
The dataset is divided into training and test sets
based on three ratios include 60-40\%, 70-30\%, and 80-20\%. The training set is used to create the model, and its performance is evaluated using the test set.\\
To improve the dataset quality, certain preprocessing techniques are applied, including data cleansing, feature selection, data reduction, and data transformation.\\
\textbf{b) Evaluation:}\\
In this research, predictive models are evaluated according to three criteria: RMSE, MAPE, MAE, and Huber Loss.\\ 
In the following formulas:\\
\textbullet n is the number of observations.
\textbullet $X_i$ element is the predicted ith value.
\textbullet $Y_i$ element is the actual ith value.\\
- Root Mean Squared Error - RMSE:\\
\begin{equation}
    RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}{(x_{i}-y_{i})}^2}
\end{equation}
(Best value = 0; worst value = +$\propto$)\\
- Mean Absolute Percentage Error – MAPE:\\
\begin{equation}
    MAPE = \frac{1}{n}\sum_{i=1}^{n}{(x_{i}-\bar{x})}|\frac{y_i - x_i}{y_i}|
\end{equation}
(Best value = 0; worst value = +$\propto$)\\
- Mean Absolute Error - MAE:\\
\begin{equation}
    MAE = \frac{1}{n}\sum_{i=1}^{n}|X_i - Y_i|
\end{equation}
(Best value = 0; worst value = +$\propto$)\\
- Huber Loss:
\begin{equation}
    L_{\delta}(y,f(x)) = \begin{cases}
    \frac {1}{2} {(y-f(x))}^{2}\\
    {\delta} |y-f(x)| - \frac {1}{2} {\delta}^{2}
    \end{cases}
\end{equation}
(Best value = 0; worst value = +$\propto$)\\
\\
\textbf{c) Results:}\\
\begin{table}[H]
\centering
\caption{Honda Dataset}
\resizebox{0.5\textwidth}{!}
{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Model & Proportion & RMSE & MAPE & MAE & Huber Loss \\
\hline
\multirow{Linear regression} & 6-4 & 587.59 & 0.35\% & 441.13 & 345264.36 \\
 & 7-3 & 618.56 & 0.35\% & 457.37 & 382622.29 \\
 & 8-2 & 653.17 & 0.34\% & 471.38 & 426635.47 \\
\hline
\multirow{ARIMA} & 6-4 & 192.729 & 10.413\% & 138.902 & 138.402 \\
 & 7-3 & 318.375 & 17.26\% & 242.088 & 241.588 \\
 & 8-2 & 334.586 & 17.283\% & 256.901 & 256.401 \\
\hline
\multirow{SVR} & 6-4 & 202.6 & 5.74\% & 89.6 & 41077.7 \\
 & 7-3 & 232.4 & 7.34\% & 115.6 & 54034.02 \\
 & 8-2 & 287.4 & 10.94\% & 173.3 & 82609.60 \\
\hline
\multirow{RNN} & 6-4 & 3047.4376 & 1.6752\% & 2209.5247 & 2209.0247 \\
 & 7-3 & 2977.5322 & 1.5669\% & 2167.6648 & 2167.16482 \\
 & 8-2 & 4499.2708 & 2.4328\% & 3656.5120 & 3656.0120 \\
\hline
\multirow{GARCH} & 6-4 & 2.361 & 444.088\% & 1.931 & 1.475 \\
 & 7-3 & 2.312 & 430.566\% & 1.881 & 1.426 \\
 & 8-2 & 2.243 & 401.801\% & 1.811 & 1.36 \\
\hline
\multirow{CNN-GRU} & 6-4 & 51.4 & 3.18\% & 39.8 & 0.0027 \\
 & 7-3 & 64.9 & 4.06\% & 53.3 & 0.0035 \\
 & 8-2 & 64.7 & 3.37\% & 49.6 & 0.0024 \\
\hline
\multirow{Transformer} & 6-4 & 110037.93 & 40794230.4\% & 108679.92 & 108678.92 \\
 & 7-3 & 109859.21 & 39739681.32\% & 108340.88 & 108339.89 \\
 & 8-2 & 111171.1 & 38203843.81\% & 109876.34 & 109875.34 \\
\hline
\multirow{DNN} & 6-4 &  10.971 & 0.654\% & 8.727 & 8.231 \\
 & 7-3 & 0.335 & 0.022\% & 0.272 & 0.056 \\
 & 8-2 & 5.427 & 0.303\% & 4.406 & 3.916 \\

\hline
\end{tabular}
}
\end{table} 
Based on the table we conclude that the best model for forecasting the next 30 days stock price of Honda datasets is DNN model with the proportion 7:3 because it has the lowest RMSE, MAPE, MAE, and Huber Loss values.
\begin{table}[H]
\centering
\caption{Suzuki Dataset}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Model & Proportion & RMSE & MAPE & MAE & Huber Loss \\
\hline
\multirow{Linear regression} & 6-4 & 2622.02 & 0.37\% & 1953.77 & 6874988.22 \\
 & 7-3 & 2538.34 & 0.40\% & 1930.07 & 6443157.89 \\
 & 8-2 & 2622.02 & 0.37\% & 1953.77 & 6874988.22 \\
\hline
\multirow{ARIMA} & 6-4 & 706.891 & 10.731\% & 549.207 & 548.707 \\
 & 7-3 & 1413.519 & 24.456\% & 1252.374 & 1251.874 \\
 & 8-2 & 814.918 & 11.23\% & 623.048 & 622.548 \\
\hline
\multirow{SVR} & 6-4 & 45.3 & 0.67\% & 32.4 & 2053.72 \\
 & 7-3 & 42.2 & 0.6\% & 30.3 & 1784.01 \\
 & 8-2 & 45.9 & 0.61\% & 31.7 & 2108.3 \\
\hline
\multirow{RNN} & 6-4 & 9335.2988 & 1.46\% & 6946.0742 & 6945.5742 \\
 & 7-3 & 9291.3207 & 1.3396\% & 6848.3873 & 6847.8873 \\
 & 8-2 & 9449.6151 & 1.3448\% & 7244.7655 & 7244.2655 \\
\hline
\multirow{GARCH} & 6-4 & 2.711 & 526.356\% & 2.2 & 1.738 \\
 & 7-3 & 2.548 & 559.877\% & 2.097 & 1.635 \\
 & 8-2 & 2.413 & 570.121\% & 2.015 & 1.554 \\
\hline
\multirow{CNN-GRU} & 6-4 & 212.4 & 4.32\% & 200.9 & 0.00284349 \\
 & 7-3 & 213.8 & 3.38\% & 172.5 & 0.0020438 \\
 & 8-2 & 212.4 & 3.21\% & 170.5 & 0.00174542 \\
\hline
\multirow{Transformer} & 6-4 & 500795.24 & 142294777.79\% & 494680.06 & 494679.06 \\
 & 7-3 & 493206.60 & 120073873.45\% & 487288.66 & 487287.66 \\
 & 8-2 & 504821.35 & 119882334.43\% & 498710.54 & 498709.54 \\
\hline
\multirow{DNN} & 6-4 & 30.272 & 0.544\% & 25.539 & 25.047 \\
 & 7-3 & 19.102 & 0.27\% & 14.101 & 13.602 \\
 & 8-2 & 21.363 & 0.313\% & 17.086 & 16.592 \\
\hline
\end{tabular}
}
\end{table}\\

Based on the table we conclude that the best model for forecasting the next 30 days stock price of Honda datasets is DNN model with the proportion 7:3 because it has the lowest RMSE, MAPE, MAE, and Huber Loss values.
\begin{table}[H]
\centering
\caption{Yamaha Dataset}
\resizebox{0.5\textwidth}{!}{
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Model & Proportion & RMSE & MAPE & MAE & Huber Loss \\
\hline
\multirow{Linear regression} & 6-4 & 2700.82 & 0.38\% & 2047.49 & 7294411.09 \\
 & 7-3 & 2333.01 & 0.36\% & 1824.93 & 5442929.77 \\
 & 8-2 & 2130.75 & 0.34\% & 1652.80 & 4540103.35 \\
\hline
\multirow{ARIMA} & 6-4 & 1152.083 & 20.711\% & 988.014 & 987.514 \\
 & 7-3 & 610.2 & 10.54\% & 485.33 & 484.836 \\
 & 8-2 & 739.748 & 12.777\% & 531.663 & 531.167 \\
\hline
\multirow{SVR} & 6-4 & 372.7 & 2.67\% & 143.5 & 138935.59 \\
 & 7-3 & 211.4 & 1.95\% & 75.1 & 44725.6 \\
 & 8-2 & 282.1 & 2.83\% & 103.6 & 79623.6 \\
\hline
\multirow{RNN} & 6-4 & 14881.8682 & 2.3251\% & 11196.8949 & 11196.3948 \\
 & 7-3 & 11887.1515 & 1.8766\% & 8903.7951 & 8903.2951 \\
 & 8-2 & 12497.5631 & 1.9696\% & 8671.0388 & 8670.5388 \\
\hline
\multirow{GARCH} & 6-4 & 2.721 & 368.168\% & 2.211 & 1.746 \\
 & 7-3 & 2.71 & 385.272\% & 2.181 & 1.716 \\
 & 8-2 & 2.7 & 430.017\% & 2.139 & 1.68 \\
\hline
\multirow{CNN-GRU} & 6-4 & 345.2 & 5.82\% & 271.6 & 0.007 \\
 & 7-3 & 340.2 & 5.8\% & 253.4 & 0.006 \\
 & 8-2 & 419.7 & 8.29\% & 347.5 & 0.009 \\
\hline
\multirow{Transformer} & 6-4 & 529087.04 & Infinity & 524507.04 & 524506.04 \\
 & 7-3 & 549296.05 & 136289644.55\% & 543958.72 & 543957.73 \\
 & 8-2 & 550585.94 & 136858355.56\% & 546228.56 & 546227.57 \\
\hline
\multirow{DNN} & 6-4 & 17.687 & 0.194\% & 11.037 & 10.554 \\
 & 7-3 & 11.924 & 0.224\% & 10.697 & 10.210 \\
 & 8-2 & 38.558 & 0.732\% & 31.355 & 30.86 \\

\hline
\end{tabular}
}
\end{table}\\
\\
Based on the table we conclude that the best model for forecasting the next 30 days stock price of Yamaha datasets is DNN model with the proportion 7:3 because it has the lowest RMSE, MAPE, MAE, and Huber Loss values.\\
\\
\textbf{d) Visualize:}\\
Visualizing the predicted values and the actual values of the DNN model on Honda dataset and the next 30 days forecasting values.
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Honda_predict.png}
    \caption{Predictions of the DNN model with Honda dataset and rate of 7:3}
    \label{fig:enter-label}
\end{figure}
Visualizing the predicted values and the actual values of the DNN model on Suzuki dataset and the next 30 days forecasting values.
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Suzuki_predict.png}
    \caption{Predictions of the DNN model with Suzuki dataset and rate of 7:3}
    \label{fig:enter-label}
\end{figure}
Visualizing the predicted values and the actual values of the DNN model on Yamaha dataset and the next 30 days forecasting values.
\begin{figure} [H]
    \centering
    \includegraphics[width=1\linewidth]{Yamaha_predict.png}
    \caption{Predictions of the DNN model with Yamaha dataset and rate of 7:3}
    \label{fig:enter-label}
\end{figure}



\section{Conclusion}
With remarkable advancements in machine learning and deep learning techniques, these methodologies have become indispensable tools for forecasting. In this research, we employed eight different machine learning models to analyze and predict stock prices of three Japanese motor companies: Honda, Suzuki, and Yamaha. Utilizing metrics such as Mean Absolute Percentage Error, Root Mean Square Error, Mean Absolute Error, and Huber Loss, we evaluated the performance of the models and concluded that the Deep Neural Network (DNN) model is the most suitable for forecasting stock prices of these Japanese motor companies.\\
However, applying machine learning or deep learning in this domain poses several challenges. Factors influencing stock prices, such as financial markets, political and societal aspects, and the financial results of the company, add complexity to the prediction task. To address these challenges, a practical approach is to leverage the strengths of each method while mitigating their respective weaknesses. For instance, combining two different models, Support Vector Machine (SVM) and Long Short-Term Memory (LSTM), to predict stock prices involves using SVM to forecast future stock prices based on easily measurable and linearly correlated variables, and utilizing LSTM to predict stock prices based on the temporal dynamics and nonlinear relationships. The combination of SVM and LSTM can balance linear and nonlinear predictive capabilities, harnessing the strengths of both models to provide accurate and multidimensional stock price predictions.\\
In the future, we aim to strengthen and supplement the shortcomings not adequately addressed in this report. We will strive to apply additional methods to enhance the accuracy of forecasting models. Additionally, expanding the dataset, integrating supplementary data sources such as financial news, global events, or economic indices, and adopting new machine learning and deep learning techniques will enrich the prediction process. By implementing these improvements, the research can not only elevate the accuracy of predictions but also open opportunities to gain deeper insights into the factors influencing financial markets.
\section*{Acknowledgment}
We would like to express our deep gratitude to Associate Professor. Teacher Nguyen Dinh Thuan, T.A Nguyen Minh Nhut and T.A Nguyen Thi Viet Huong spent your time and dedication to guide us in the process of implementing the subject project. Your valuable suggestions and support have helped us complete the project excellently. We are very grateful and proud to have the opportunity to study under your guidance. Thank you teacher for your great and meaningful contribution to our learning process. We hope that we will continue to receive support and encouragement from you in the future.
%% these lines used to import a separate ".bib" for the bibliografy.
\bibliographystyle{plain}
\bibliography{team11bib}

%% UNCOMMENT these lines below (and remove the 2 commands above) if you want to embed the bibliografy.
%\begin{thebibliography}{00}
%\bibitem{b1} G. O. Young, ``Synthetic structure of industrial plastics,'' in \emph{Plastics,} 2\textsuperscript{nd} ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64. 
%\bibitem{b2} W.-K. Chen, \emph{Linear Networks and Systems.} Belmont, CA, USA: Wadsworth, 1993, pp. 123--135. 
%\bibitem{b3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' \emph{IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, 10.1109/TED.2016.2628402. 
%\bibitem{b4} E. P. Wigner, ``Theory of traveling-wave optical laser,'' \emph{Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965. 
%\bibitem{b5} E. H. Miller, ``A note on reflector arrays,'' \emph{IEEE Trans. Antennas Propagat}., to be published. 
%\bibitem{b6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988. 
%\bibitem{b7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987. 
%\bibitem{b8} \emph{Transmission Systems for Communications}, 3\textsuperscript{rd} ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60. 
%\bibitem{b9} \emph{Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989. 
%\bibitem{b10} G. O. Young, ``Synthetic structure of industrial 
%plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters, 
%Ed., 2\textsuperscript{nd} ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64. 
%[Online]. Available: 
%\underline{http://www.bookref.com}. 
%\bibitem{b11} \emph{The Founders' Constitution}, Philip B. Kurland 
%and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987. 
%[Online]. Available: \underline{http://press-pubs.uchicago.edu/founders/} 
%\bibitem{b12} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014. 
%[Online]. Available: 
%\underline{http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf}. Accessed on: May 19, 2014. 
%\bibitem{b13} Philip B. Kurland and Ralph Lerner, eds., \emph{The 
%	Founders' Constitution.} Chicago, IL, USA: Univ. of Chicago Press, 
%1987, Accessed on: Feb. 28, 2010, [Online] Available: 
%\underline{http://press-pubs.uchicago.edu/founders/} 
%\bibitem{b14} J. S. Turner, ``New directions in communications,'' \emph{IEEE J. Sel. Areas Commun}., vol. 13, no. 1, pp. 11-23, Jan. 1995. 
%\bibitem{b15} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' \emph{Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986. 
%\bibitem{b16} P. Kopyt \emph{et al., ``}Electric properties of graphene-based conductive layers from DC up to terahertz range,'' \emph{IEEE THz Sci. Technol.,} to be published. DOI: 10.1109/TTHZ.2016.2544142. 
%\bibitem{b17} PROCESS Corporation, Boston, MA, USA. Intranets: 
%Internet technologies deployed behind the firewall for corporate 
%productivity. Presented at INET96 Annual Meeting. [Online]. 
%Available: \underline{http://home.process.com/Intranets/wp2.htp} 
%\bibitem{b18} R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available: \underline {http://CRAN.R-project.org/package=raster}  
%\bibitem{b19} Teralyzer. Lytera UG, Kirchhain, Germany [Online]. 
%Available: 
%\underline{http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id=home}, Accessed on: Jun. 5, 2014 
%\bibitem{b20} U.S. House. 102\textsuperscript{nd} Congress, 1\textsuperscript{st} Session. (1991, Jan. 11). \emph{H. Con. Res. 1, Sense of the Congress on Approval of}  \emph{Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS 
%\bibitem{b21} Musical toothbrush with mirror, by L.M.R. Brooks. (1992, May 19). Patent D 326 189 [Online]. Available: NEXIS Library: LEXPAT File: DES 
%\bibitem{b22} D. B. Payne and J. R. Stern, ``Wavelength-switched pas- sively coupled single-mode optical network,'' in \emph{Proc. IOOC-ECOC,} Boston, MA, USA, 1985, pp. 585--590. 
%\bibitem{b23} D. Ebehard and E. Voges, ``Digital single sideband detection for interferometric sensors,'' presented at the \emph{2\textsuperscript{nd} Int. Conf. Optical Fiber Sensors,} Stuttgart, Germany, Jan. 2-5, 1984. 
%\bibitem{b24} G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978. 
%\bibitem{b25} J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993. 
%\bibitem{b26} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993. 
%\bibitem{b27} A. Harrison, private communication, May 1995. 
%\bibitem{b28} B. Smith, ``An approach to graphs of linear forms,'' unpublished. 
%\bibitem{b29} A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85. 
%\bibitem{b30} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969. 
%\bibitem{b31} Letter Symbols for Quantities, ANSI Standard Y10.5-1968. 
%\bibitem{b32} R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' \emph{Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103.~ 
%\bibitem{b33} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' \emph{IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111 
%\bibitem{b34} S. Azodolmolky~\emph{et al.}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,''~\emph{J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011.
%\end{thebibliography}
%%%%%%%%%%%%%%%

\EOD

\end{document}
